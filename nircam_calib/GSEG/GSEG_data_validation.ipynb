{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSEG data validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code that can be used to validate data created during GSEG3. It reads in the APT-derived xml and pointing files and constructs a dictionary of expected data properties. It then compares these properties to the information contained in the headers of the actual data to look for inconsistencies.\n",
    "\n",
    "**Mirage and pysiaf are dependencies. Be sure they are installed in your environment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from astropy.io import fits\n",
    "from mirage.yaml import yaml_generator\n",
    "from mirage.apt import apt_inputs\n",
    "from mirage.seed_image.catalog_seed_image import Catalog_seed\n",
    "from mirage.utils.siaf_interface import sci_subarray_corners\n",
    "from mirage.utils.utils import calc_frame_time\n",
    "from mirage.yaml.generate_observationlist import get_observation_dict\n",
    "import numpy as np\n",
    "import pkg_resources\n",
    "import pysiaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header keywords to check against APT-derived dictionary\n",
    "# It is assumed that these header keywords exist in both the uncal and rate images\n",
    "KEYWORDS = ['SUBARRAY', 'DETECTOR', 'NINTS', 'NGROUPS', 'NAXIS', 'EFFEXPTM',\n",
    "            'LONGFILTER', 'LONGPUPIL', 'SHORTFILTER', 'SHORTPUPIL', 'READPATT',\n",
    "            'OBSLABEL', 'EXP_TYPE', 'TITLE', 'OBSERVTN', 'TEMPLATE',\n",
    "            'EXPRIPAR', 'SUBSTRT1', 'SUBSTRT2', 'SUBSIZE1', 'SUBSIZE2',\n",
    "            'FASTAXIS', 'SLOWAXIS', 'PATTTYPE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corresponding keywords in the APT-derived dictionary\n",
    "# These must correspond one-to-one with KEYWORDS above\n",
    "TABLE_KEYWORDS = ['Subarray', None, 'Integrations', 'Groups', None, None,\n",
    "                  'LongFilter', 'LongPupil', 'ShortFilter', 'ShortPupil', 'ReadoutPattern',\n",
    "                  'ObservationName', 'Mode', 'Title', 'ObservationID', 'APTTemplate',\n",
    "                  'ParallelInstrument', None, None, None, None,\n",
    "                  None, None, 'PrimaryDitherType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of header keywords and their expected values in the \n",
    "# PRIMARY and SCI extensions of the UNCAL and RATE files.\n",
    "# These are useful when you know exactly what these keywords should be\n",
    "# for every file (e.g. NAXIS should always be 2 in the RATE image SCI header).\n",
    "# These will be checked in addition to the KEYWORDS checks above\n",
    "UNCAL_PRIMARY_KEYWORDS = {}\n",
    "UNCAL_SCI_KEYWORDS = {'BITPIX':16, 'NAXIS':4, 'BUNIT':'DN'}\n",
    "RATE_PRIMARY_KEYWORDS = {}\n",
    "RATE_SCI_KEYWORDS = {'BITPIX':-32, 'NAXIS':2, 'BUNIT':'DN/s'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTEGER_KEYWORDS = ['Integrations', 'Groups']\n",
    "FLOAT_KEYWORDS = ['EFFEXPTM']\n",
    "FILTER_KEYWORDS = ['LONGFILTER', 'LONGPUPIL', 'SHORTFILTER', 'SHORTPUPIL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For FASTAXIS and SLOWAXIS\n",
    "HORIZONTAL_FLIP = ['NRCA1', 'NRCA3', 'NRCALONG', 'NRCB2', 'NRCB4']\n",
    "VERTICAL_FLIP = ['NRCA2', 'NRCA4', 'NRCB1', 'NRCB3', 'NRCBLONG']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_exptype(value):\n",
    "    \"\"\"Modify the exposure type as listed in the exposure table\n",
    "    to match one of the strings as used in the fits files.\n",
    "    e.g. 'imaging' becomes 'NRC_IMAGE'\n",
    "    Remember that currently, Mirage only knows imaging and wfss\n",
    "    \"\"\"\n",
    "    if value == 'imaging':\n",
    "        return 'NRC_IMAGE'\n",
    "    elif value == 'wfss':\n",
    "        return 'NRC_GRISM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_files(exp_dict, index):\n",
    "    \"\"\"Calculate the total number of files expected for an\n",
    "    observation based on the number of dithers and the module.\n",
    "    ASSUME that all detectors in a given module are used. This\n",
    "    assumption will not be true for some WFSC apertures.\n",
    "    \"\"\"\n",
    "    module = exp_dict['Module'][index]\n",
    "    number_of_dithers = exp_dict['number_of_dithers'][index]\n",
    "    if module in ['A', 'B']:\n",
    "        dets = 5\n",
    "    else:\n",
    "        dets = 10\n",
    "    total = number_of_dithers * dets\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize_file_lists(uncal, rate):\n",
    "    \"\"\"Given lists of uncal and rate files corresponding to a single\n",
    "    observation, adjust the lists to be the same length, adding in\n",
    "    None for any files that are missing in a given list\n",
    "    \"\"\"\n",
    "    udict = {}\n",
    "    rdict = {}\n",
    "    expanded_rate = []\n",
    "    expanded_uncal = []\n",
    "\n",
    "    # Loop through uncal files and look for matching rate files\n",
    "    for ufile in uncal:\n",
    "        dirname, filename = os.path.split(ufile)\n",
    "        base = filename.strip('_uncal.fits')\n",
    "        fullbase = os.path.join(dirname, base)\n",
    "        found = False\n",
    "        for rfile in rate:\n",
    "            if fullbase in rfile:\n",
    "                found = True\n",
    "                break\n",
    "        udict[base] = found\n",
    "\n",
    "    # Loop through rate files and look for matching uncal files\n",
    "    for rfile in rate:\n",
    "        dirname, filename = os.path.split(rfile)\n",
    "        base = filename.strip('_rate.fits')\n",
    "        fullbase = os.path.join(dirname, base)\n",
    "        found = False\n",
    "        for ufile in uncal:\n",
    "            if fullbase in ufile:\n",
    "                found = True\n",
    "                break\n",
    "        rdict[base] = found\n",
    "\n",
    "    # Fill in missing files, in either uncal or rate lists,\n",
    "    # with None\n",
    "    for ukey in udict:\n",
    "        expanded_uncal.append(ukey + '_uncal.fits')\n",
    "        if udict[key]:\n",
    "            expanded_rate.append(ukey + '_rate.fits')\n",
    "        else:\n",
    "            expanded_rate.append(None)\n",
    "    for rkey in rdict:\n",
    "        if not rdict[key]:\n",
    "            expanded_rate.append(rkey + '_rate.fits')\n",
    "            expanded_uncal.append(None)\n",
    "    return expanded_uncal, expanded_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_existing_files(uncal_files, rate_files):\n",
    "    \"\"\"Find the number of uncal and rate files that exist\n",
    "    in the input file paths, and return any missing uncal/rate files.\n",
    "    The expectation is that every uncal file has a corresponding \n",
    "    rate file.\n",
    "    \"\"\"\n",
    "    uncal_files_exist = np.array([os.path.isfile(f) for f in uncal_files])\n",
    "    rate_files_exist = np.array([os.path.isfile(f) for f in rate_files])\n",
    "    \n",
    "    n_uncal = len(uncal_files_exist[uncal_files_exist])\n",
    "    n_rate = len(rate_files_exist[rate_files_exist])\n",
    "    \n",
    "    missing_uncal_files = np.array(uncal_files)[~uncal_files_exist]\n",
    "    missing_rate_files = np.array(rate_files)[~rate_files_exist]\n",
    "    \n",
    "    return n_uncal, n_rate, missing_uncal_files, missing_rate_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fastaxis(detector):\n",
    "    \"\"\"Identify the values of FASTAXIS and SLOWAXIS based on the detector\n",
    "    name\n",
    "    \"\"\"\n",
    "    if detector in HORIZONTAL_FLIP:\n",
    "        fast = -1\n",
    "        slow = 2\n",
    "    elif detector in VERTICAL_FLIP:\n",
    "        fast = 1\n",
    "        slow = -2\n",
    "    return fast, slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    \"\"\"Read in the given fits file and return the data and header\n",
    "    \"\"\"\n",
    "    with fits.open(filename) as h:\n",
    "        signals = h['SCI'].data\n",
    "        header0 = h[0].header\n",
    "        header1 = h[1].header\n",
    "    return signals, header0, header1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_shape(sub):\n",
    "    \"\"\"Returns the expected shape of the science data\n",
    "    based on the input APT subarray.\n",
    "    \"\"\"\n",
    "    siaf = pysiaf.Siaf('NIRCam')\n",
    "    subarray = sub.replace('SUB', '').replace('DHSPILA', '').replace('DHSPILB', '')\n",
    "    \n",
    "    if 'FULL' in subarray:\n",
    "        expected_shape = (2048, 2048)\n",
    "    else:\n",
    "        # needed to be careful here to remove cases where e.g. SUB64 was in SUB640\n",
    "        similar_aps = [aper for aper in siaf.apernames if subarray in aper and subarray+'0' not in aper]\n",
    "        if len(similar_aps) == 0:\n",
    "            print('WARNING: Cannot find expected shape for subarray {}'.format(sub))\n",
    "            expected_shape = (-99, -99)\n",
    "        else:\n",
    "            # just use first entry to get expected shape since they should all be the same\n",
    "            similar_ap = similar_aps[0]\n",
    "            expected_shape = (siaf[similar_ap].YSciSize, siaf[similar_ap].XSciSize)\n",
    "    \n",
    "    return expected_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def header_keywords(head):\n",
    "    \"\"\"Extract values for the desired keywords from the given header\n",
    "    \"\"\"\n",
    "    file_info = {}\n",
    "    for keyword in KEYWORDS:\n",
    "        try:\n",
    "            info = head[keyword]\n",
    "        except KeyError:\n",
    "            if 'FILTER' in keyword:\n",
    "                info = head['FILTER']\n",
    "            elif 'PUPIL' in keyword:\n",
    "                info = head['PUPIL']\n",
    "            else:\n",
    "                info = None\n",
    "\n",
    "        file_info[keyword] = info\n",
    "    return file_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_info(values, index):\n",
    "    \"\"\"Extract information from the exposure table that matches the\n",
    "    header keyword values in KEYWORDS\n",
    "    \"\"\"\n",
    "    values_dict = {}\n",
    "    for table_keyword, file_keyword in zip(TABLE_KEYWORDS, KEYWORDS):\n",
    "        if table_keyword is not None:\n",
    "            if table_keyword in INTEGER_KEYWORDS:\n",
    "                value = int(values[table_keyword][index])\n",
    "            else:\n",
    "                value = values[table_keyword][index]\n",
    "            values_dict[file_keyword] = value\n",
    "        else:\n",
    "            values_dict[file_keyword] = None\n",
    "    return values_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_dimensions(filename, file_type, expected_shape):\n",
    "    \"\"\"Verify the header and data dimensions for each extension.\n",
    "    \"\"\"\n",
    "    header = fits.getheader(filename, 'PRIMARY')\n",
    "    if file_type == 'UNCAL':\n",
    "        extensions = ['SCI']\n",
    "        primary_header_shape = (header['NINTS'], header['NGROUPS'], header['SUBSIZE2'], header['SUBSIZE1'])\n",
    "        for ext in extensions:\n",
    "            try:\n",
    "                header = fits.getheader(filename, ext)\n",
    "                data_shape = fits.getdata(filename, ext).shape\n",
    "                naxis_shape = (header['NAXIS4'], header['NAXIS3'], header['NAXIS2'], header['NAXIS1'])\n",
    "                if ((primary_header_shape != data_shape) | (primary_header_shape != naxis_shape) | \n",
    "                    (primary_header_shape[-2:] != expected_shape)):\n",
    "                    print('WARNING: Data dimensions incorrect')\n",
    "                    print('Expected image shape: {}'.format(expected_shape))\n",
    "                    print('PRIMARY header shape: {}'.format(primary_header_shape))\n",
    "                    print('{} header shape: {}'.format(ext, naxis_shape))\n",
    "                    print('{} data shape: {}'.format(ext, data_shape)) \n",
    "            except KeyError:\n",
    "                print('Cannot verify shape for {} extension'.format(ext))\n",
    "    elif file_type == 'RATE':\n",
    "        extensions = 'SCI ERR DQ VAR_POISSON VAR_RNOISE'.split()\n",
    "        primary_header_shape = (header['SUBSIZE2'], header['SUBSIZE1'])\n",
    "        for ext in extensions:\n",
    "            try:\n",
    "                header = fits.getheader(filename, ext)\n",
    "                data_shape = fits.getdata(filename, ext).shape\n",
    "                naxis_shape = (header['NAXIS2'], header['NAXIS1'])\n",
    "                if ((primary_header_shape != data_shape) | (primary_header_shape != naxis_shape) | \n",
    "                    (primary_header_shape != expected_shape)):\n",
    "                    print('WARNING: Data dimensions incorrect')\n",
    "                    print('Expected image shape: {}'.format(expected_shape))\n",
    "                    print('PRIMARY header shape: {}'.format(primary_header_shape))\n",
    "                    print('{} header shape: {}'.format(ext, naxis_shape))\n",
    "                    print('{} data shape: {}'.format(ext, data_shape))\n",
    "            except KeyError:\n",
    "                print('Cannot verify shape for {} extension'.format(ext))\n",
    "    else:\n",
    "        print('File type {} not supported for dimension checks'.format(file_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_extensions(filename, file_type):\n",
    "    \"\"\"Verify that the expected extensions exist\n",
    "    \"\"\"\n",
    "    if file_type == 'UNCAL':\n",
    "        extensions = 'PRIMARY SCI GROUP INT_TIMES ASDF'.split()\n",
    "    elif file_type == 'RATE':\n",
    "        extensions = 'PRIMARY SCI ERR DQ VAR_POISSON VAR_RNOISE ASDF'.split()\n",
    "    else:\n",
    "        print('File type {} not supported for ext verification'.format(file_type))\n",
    "        \n",
    "    for ext in extensions:\n",
    "        try:\n",
    "            header = fits.getheader(filename, ext)\n",
    "        except KeyError:\n",
    "            print('WARNING: {} extension does not exist'.format(ext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(xml_file, output_dir, gseg_uncal_files):\n",
    "    \"\"\"MAIN FUNCTION\"\"\"\n",
    "    \n",
    "    read_pattern_def_file = os.path.join(pkg_resources.resource_filename('mirage', ''), \n",
    "                                         'config', 'nircam_read_pattern_definitions.list')\n",
    "    \n",
    "    # Check that the number of existing uncal and rate files are equal and\n",
    "    # that no files are missing.\n",
    "    gseg_rate_files = [f.replace('uncal', 'rate') for f in gseg_uncal_files]\n",
    "    n_uncal, n_rate, missing_uncal_files, missing_rate_files = find_existing_files(gseg_uncal_files, \n",
    "                                                                                   gseg_rate_files)\n",
    "    if (n_uncal != n_rate) | (len(missing_uncal_files) != 0) | (len(missing_rate_files) != 0):\n",
    "        print('WARNING: Some files do not exist')\n",
    "        print('Number of existing uncal files: {}'.format(n_uncal))\n",
    "        print('Number of existing rate files: {}'.format(n_rate))\n",
    "        print('Missing uncal files: {}'.format(missing_uncal_files))\n",
    "        print('Missing rate files: {}'.format(missing_rate_files))\n",
    "    \n",
    "    # Create apt-derived dictionary\n",
    "    pointing_file = xml_file.replace('.xml', '.pointing')\n",
    "    catalogs = {'nircam': {'sw': 'nothing.cat', 'lw': 'nothing.cat'}}\n",
    "\n",
    "    observation_list_file = os.path.join(output_dir, 'observation_list.yaml')\n",
    "    apt_xml_dict = get_observation_dict(xml_file, observation_list_file, catalogs,\n",
    "                                        verbose=True)\n",
    "\n",
    "    observation_list = set(apt_xml_dict['ObservationID'])\n",
    "    int_obs = sorted([int(o) for o in observation_list])\n",
    "    str_obs_list = [str(o).zfill(3) for o in int_obs]\n",
    "\n",
    "    for observation_to_check in str_obs_list:\n",
    "        print('')\n",
    "        print('')\n",
    "        print('OBSERVATION: {}'.format(observation_to_check))\n",
    "        print('')\n",
    "\n",
    "        good = np.where(np.array(apt_xml_dict['ObservationID']) == observation_to_check)\n",
    "\n",
    "        try:\n",
    "            total_expected_files = calculate_total_files(apt_xml_dict, good[0][0])\n",
    "            print('Total number of expected files: {}'.format(total_expected_files))\n",
    "        except IndexError:\n",
    "            print(\"No files found.\")\n",
    "            continue\n",
    "\n",
    "        # The complication here is that the table created by Mirage does not have a filename\n",
    "        # attached to each entry. So we need a way to connect an actual filename\n",
    "        # to each entry\n",
    "        subdir_start = 'jw' + apt_xml_dict['ProposalID'][good[0][0]] + observation_to_check.zfill(3)\n",
    "        matching_uncal_files = sorted([filename for filename in gseg_uncal_files if subdir_start in filename])\n",
    "        matching_rate_files = sorted([filename for filename in gseg_rate_files if subdir_start in filename])\n",
    "        print('Found uncal files:')\n",
    "        for i in range(len(matching_uncal_files)):\n",
    "            print(matching_uncal_files[i])\n",
    "        print('')\n",
    "        print('Found rate files:')\n",
    "        for i in range(len(matching_rate_files)):\n",
    "            print(matching_rate_files[i])\n",
    "        print('')\n",
    "\n",
    "        # Check to see if any files are missing\n",
    "        if len(matching_uncal_files) != total_expected_files:\n",
    "            print(\"WARNING: Missing uncal files for observation {}. Expected {} files, found {}.\".format(observation_to_check, total_expected_files, len(matching_uncal_files)))\n",
    "        if len(matching_rate_files) != total_expected_files:\n",
    "            print(\"WARNING: Missing rate files for observation {}. Expected {} files, found {}.\".format(observation_to_check, total_expected_files, len(matching_rate_files)))\n",
    "\n",
    "        # Deal with the case of matching_uncal_files and matching_rate_files having\n",
    "        # different lengths here. In order to loop over them they must have the same length\n",
    "        if len(matching_uncal_files) != len(matching_rate_files):\n",
    "            (matching_uncal_files, matching_rate_files) = equalize_file_lists(matching_uncal_files, matching_rate_files)\n",
    "            print('Equalized file lists (should have a 1:1 correspondence):')\n",
    "            for idx in range(len(matching_uncal_files)):\n",
    "                print(matching_uncal_files[idx], matching_rate_files[idx])\n",
    "\n",
    "        # Create siaf instance for later calculations\n",
    "        siaf = pysiaf.Siaf('NIRCam')\n",
    "\n",
    "        for file_pair in zip(matching_uncal_files, matching_rate_files):\n",
    "            for f in file_pair: \n",
    "                # Only validate files that exist\n",
    "                good_file = f != None\n",
    "                if good_file:\n",
    "                    if not os.path.isfile(f):\n",
    "                        print('WARNING: File does not exist: {}'.format(f))\n",
    "                        good_file = False\n",
    "                \n",
    "                if good_file:\n",
    "                    print(\"Checking {}\".format(os.path.split(f)[1]))\n",
    "                    print('-----------------------------------------------')\n",
    "                    file_type = f.split('.fits')[0].split('_')[-1].upper()\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                data, header, sci_header = get_data(f)\n",
    "                \n",
    "                # Get info from header to be compared\n",
    "                header_vals = header_keywords(header)\n",
    "\n",
    "                # Get matching data from the exposure table\n",
    "                table_vals = table_info(apt_xml_dict, good[0][0])\n",
    "                \n",
    "                # Verify that all expected extensions exist for this file\n",
    "                verify_extensions(f, file_type)\n",
    "                \n",
    "                # Verify that the header and data dimensions are correct in each extension\n",
    "                expected_shape = get_expected_shape(table_vals['SUBARRAY'])\n",
    "                verify_dimensions(f, file_type, expected_shape)\n",
    "                \n",
    "                # Check detector/aperture\n",
    "                detector_from_filename = f.split('_')[-2].upper()\n",
    "                header_detector = header['DETECTOR']\n",
    "                aperture = header['APERNAME']  # could also try APERNAME, PPS_APER\n",
    "                if 'LONG' in header_detector:\n",
    "                    header_detector = header_detector.replace('LONG', '5')\n",
    "                if header_detector not in aperture:\n",
    "                    print((\"WARNING: Detector name and aperture name in file header appear to be incompatible: {}, {}\"\n",
    "                          .format(header['DETECTOR'], aperture)))\n",
    "                    print(\"Detector listed in filename: {}\".format(detector_from_filename))\n",
    "                    print('If the aperture is incorrect then the calculated subarray location from pysiaf will also be incorrect.')\n",
    "                data_shape = data.shape\n",
    "                \n",
    "                # Compare NFRAME, GROUPGAP from header with expected values based on READPATT\n",
    "                m = Catalog_seed()\n",
    "                params = {'Readout': {'readpatt': header['READPATT']},\n",
    "                          'Reffiles': {'readpattdefs': read_pattern_def_file}}\n",
    "                m.params = params\n",
    "                m.read_pattern_check()\n",
    "                nframes = m.params['Readout']['nframe']\n",
    "                groupgap = m.params['Readout']['nskip']\n",
    "                if nframes != header['NFRAMES']:\n",
    "                    print('WARNING: NFRAME mismatch between header ({}) and expected value ({}).'.format(\n",
    "                          nframes, header['NFRAMES']))\n",
    "                if groupgap != header['GROUPGAP']:\n",
    "                    print('WARNING: GROUPGAP mismatch between header ({}) and expected value ({}).'.format(\n",
    "                          groupgap, header['GROUPGAP']))\n",
    "\n",
    "                # Make some adjustments to the exposure table info\n",
    "\n",
    "                # Calucate the exposure time\n",
    "                print('Aperture listed in header is: {}'.format(aperture))\n",
    "                num_amps = 1\n",
    "                frametime = calc_frame_time('NIRCam', aperture, data_shape[-1], data_shape[-2], num_amps)\n",
    "                table_vals['EFFEXPTM'] = frametime * int(table_vals['NGROUPS'])\n",
    "\n",
    "                # NAXIS\n",
    "                table_vals['NAXIS'] = len(data.shape)\n",
    "                header_vals['NAXIS'] = sci_header['NAXIS']\n",
    "\n",
    "                # Use pysiaf to calculate subarray locations\n",
    "                try:\n",
    "                    xc, yc = sci_subarray_corners('NIRCam', aperture, siaf=siaf)\n",
    "                    table_vals['SUBSTRT1'] = xc[0] + 1\n",
    "                    table_vals['SUBSTRT2'] = yc[0] + 1\n",
    "                    table_vals['SUBSIZE1'] = siaf[aperture].XSciSize\n",
    "                    table_vals['SUBSIZE2'] = siaf[aperture].YSciSize\n",
    "                except KeyError:\n",
    "                    print(\"ERROR: Aperture {} is not a valid aperture in pysiaf.\".format(aperture))\n",
    "                    xc = [-2, -2]\n",
    "                    yc = [-2, -2]\n",
    "                    table_vals['SUBSTRT1'] = xc[0] + 1\n",
    "                    table_vals['SUBSTRT2'] = yc[0] + 1\n",
    "                    table_vals['SUBSIZE1'] = 9999\n",
    "                    table_vals['SUBSIZE2'] = 9999\n",
    "\n",
    "                # Create FASTAXIS and SLOWAXIS values based on the detector name\n",
    "                fast, slow = find_fastaxis(header_vals['DETECTOR'])\n",
    "                table_vals['FASTAXIS'] = fast\n",
    "                table_vals['SLOWAXIS'] = slow\n",
    "\n",
    "                # Remove whitespace from observing template in file\n",
    "                header_vals['TEMPLATE'] = header_vals['TEMPLATE'].replace(' ', '').lower()\n",
    "                table_vals['TEMPLATE'] = table_vals['TEMPLATE'].lower()\n",
    "\n",
    "                # Adjust prime/parallel boolean from table to be a string\n",
    "                if not table_vals['EXPRIPAR']:\n",
    "                    table_vals['EXPRIPAR'] = 'PRIME'\n",
    "                else:\n",
    "                    table_vals['EXPRIPAR'] = 'PARALLEL'\n",
    "\n",
    "                # Change exposure type from table to match up with\n",
    "                # types of strings in the file\n",
    "                table_vals['EXP_TYPE'] = adjust_exptype(table_vals['EXP_TYPE'])\n",
    "\n",
    "                # Set the DETECTOR field to be identical. This info is not in the\n",
    "                # exposure table, so we can't actually check it\n",
    "                table_vals['DETECTOR'] = header_vals['DETECTOR']\n",
    "\n",
    "                # Now compare the data in the dictionary from the file versus that\n",
    "                # from the exposure table created from the APT file\n",
    "                err = False\n",
    "                for key in header_vals:\n",
    "                    if header_vals[key] != table_vals[key]:\n",
    "                        if key not in FLOAT_KEYWORDS and key not in FILTER_KEYWORDS:\n",
    "                            err = True\n",
    "                            print('MISMATCH: {}, in exp table: {}, in file: {}'.format(key, table_vals[key], header_vals[key]))\n",
    "                        elif key in FLOAT_KEYWORDS:\n",
    "                            if not np.isclose(header_vals[key], table_vals[key], rtol=0.01, atol=0.):\n",
    "                                err = True\n",
    "                                print('MISMATCH: {}, in exp table: {}, in file: {}'.format(key, table_vals[key], header_vals[key]))\n",
    "\n",
    "                        if key in ['LONGFILTER', 'LONGPUPIL'] and 'LONG' in header_vals['DETECTOR']:\n",
    "                            err = True\n",
    "                            print('MISMATCH: {}, in exp table: {}, in file: {}'.format(key, table_vals[key], header_vals[key]))\n",
    "                        if key in ['SHORTFILTER', 'SHORTPUPIL'] and 'LONG' not in header_vals['DETECTOR']:\n",
    "                            err = True\n",
    "                            print('MISMATCH: {}, in exp table: {}, in file: {}'.format(key, table_vals[key], header_vals[key]))\n",
    "                \n",
    "                # Perform direct comparison between header keywords and their expected values\n",
    "                if file_type == 'UNCAL':\n",
    "                    for key in UNCAL_PRIMARY_KEYWORDS:\n",
    "                        if UNCAL_PRIMARY_KEYWORDS[key] != header[key]:\n",
    "                            err = True\n",
    "                            print('MISMATCH: {}, expected: {}, in file: {}'.format(\n",
    "                                  key, UNCAL_PRIMARY_KEYWORDS[key], header[key]))\n",
    "                    for key in UNCAL_SCI_KEYWORDS:\n",
    "                        if UNCAL_SCI_KEYWORDS[key] != sci_header[key]:\n",
    "                            err = True\n",
    "                            print('MISMATCH: {}, expected: {}, in file: {}'.format(\n",
    "                                  key, UNCAL_SCI_KEYWORDS[key], sci_header[key]))\n",
    "                elif file_type == 'RATE':\n",
    "                    for key in RATE_PRIMARY_KEYWORDS:\n",
    "                        if RATE_PRIMARY_KEYWORDS[key] != header[key]:\n",
    "                            err = True\n",
    "                            print('MISMATCH: {}, expected: {}, in file: {}'.format(\n",
    "                                  key, RATE_PRIMARY_KEYWORDS[key], header[key]))\n",
    "                    for key in RATE_SCI_KEYWORDS:\n",
    "                        if RATE_SCI_KEYWORDS[key] != sci_header[key]:\n",
    "                            err = True\n",
    "                            print('MISMATCH: {}, expected: {}, in file: {}'.format(\n",
    "                                  key, RATE_SCI_KEYWORDS[key], sci_header[key]))\n",
    "                else:\n",
    "                    print('No direct header checks performed for {} file type.'.format(file_type))\n",
    "\n",
    "                if not err:\n",
    "                    print('No inconsistencies. File header info correct.')\n",
    "                    \n",
    "                print('')\n",
    "\n",
    "            print('')\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_file = '/path/to/proposal/xml/file/00617.xml'\n",
    "output_dir = '/location/to/place/outputs/'\n",
    "gseg_uncal_files = glob('/path/to/gseg/files/*uncal.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(xml_file, output_dir, gseg_uncal_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
